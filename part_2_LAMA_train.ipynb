{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport joblib\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom datetime import datetime\nfrom argparse import Namespace\nfrom collections import defaultdict\nfrom scipy.signal import find_peaks\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, GroupKFold, train_test_split, KFold\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 64)\n\ndef seed_everything(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-03-13T12:55:10.994631Z","iopub.execute_input":"2022-03-13T12:55:10.995006Z","iopub.status.idle":"2022-03-13T12:55:13.848078Z","shell.execute_reply.started":"2022-03-13T12:55:10.994910Z","shell.execute_reply":"2022-03-13T12:55:13.846405Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"args = Namespace(\n    train=True,\n    optimize=False,\n    train_l2=False,\n    inference=False,\n    seed=21,\n    folds=5,\n    workers=4,\n    min_time_id=None, \n    holdout=False,\n    cv_method=\"single\",\n    num_bins=16,\n    holdout_size=100,\n    outlier_threshold=0.005,\n    trading_days_per_year=250,   # chinese stock market trading days per year (roughly)\n    add_investment_id_model=False,\n    data_path=Path(\"../input/ubiquant-parquet\"),\n    just_eda=True,\n)\nseed_everything(args.seed)\n\nassert args.cv_method in {\"single\", \"kfold\", \"group\", \"stratified\", \"time\", \"group_time\", \"time_range\"}, \"unknown cv method\"\nassert args.data_path.exists(), \"data_path not exists\"","metadata":{"execution":{"iopub.status.busy":"2022-03-13T12:55:13.849961Z","iopub.execute_input":"2022-03-13T12:55:13.850221Z","iopub.status.idle":"2022-03-13T12:55:13.859789Z","shell.execute_reply.started":"2022-03-13T12:55:13.850195Z","shell.execute_reply":"2022-03-13T12:55:13.858230Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain = pd.read_parquet(args.data_path.joinpath(\"train_low_mem.parquet\"))\n# assert train.isnull().any().sum() == 0, \"null exists.\"\n# assert train.row_id.str.extract(r\"(?P<time_id>\\d+)_(?P<investment_id>\\d+)\").astype(train.time_id.dtype).equals(train[[\"time_id\", \"investment_id\"]]), \"row_id!=time_id_investment_id\"\n# assert train.time_id.is_monotonic_increasing, \"time_id not monotonic increasing\"","metadata":{"execution":{"iopub.status.busy":"2022-03-13T12:55:13.861358Z","iopub.execute_input":"2022-03-13T12:55:13.861615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features EDA + processing\n\n### Possible outliers","metadata":{}},{"cell_type":"code","source":"df = train[[\"investment_id\", \"target\"]].groupby(\"investment_id\").target.mean()\nupper_bound, lower_bound = df.quantile([1-args.outlier_threshold, args.outlier_threshold])\ndisplay(upper_bound, lower_bound)\nax = df.plot(figsize=(16, 8))\nax.axhspan(lower_bound, upper_bound, fill=False, linestyle=\"--\", color=\"k\")\nplt.show()\n\noutlier_investments = df.loc[(df>upper_bound)|(df<lower_bound)|(df==0)].index\n_=pd.pivot(\n    train.loc[train.investment_id.isin(outlier_investments), [\"investment_id\", \"time_id\", \"target\"]],\n    index='time_id', columns='investment_id', values='target'\n).plot(figsize=(16,12), subplots=True, sharex=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Drop indexes with feature outliers: [notebook](https://www.kaggle.com/junjitakeshima/ubiquant-simple-lgbm-removing-outliers-en-jp)","metadata":{}},{"cell_type":"code","source":"outlier_list = []\noutlier_col = []\n\nfor col in (f\"f_{i}\" for i in range(300)):\n    _mean, _std = train[col].mean(), train[col].std()\n    \n    temp_df = train.loc[(train[col] > _mean + _std * 70) | (train[col] < _mean - _std * 70)]\n    temp2_df = train.loc[(train[col] > _mean + _std * 35) | (train[col] < _mean - _std * 35)]\n    if len(temp_df) >0 : \n        outliers = temp_df.index.to_list()\n        outlier_list.extend(outliers)\n        outlier_col.append(col)\n        print(col, len(temp_df))\n    elif len(temp2_df)>0 and len(temp2_df) <6 :\n        outliers = temp2_df.index.to_list()\n        outlier_list.extend(outliers)\n        outlier_col.append(col)\n        print(col, len(temp2_df))\n\noutlier_list = list(set(outlier_list))\ntrain.drop(train.index[outlier_list], inplace=True)\nprint(len(outlier_col), len(outlier_list), train.shape)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop outliers from train","metadata":{}},{"cell_type":"code","source":"if args.min_time_id is not None:\n    train = train.query(\"time_id>=@args.min_time_id\").reset_index(drop=True)\n    gc.collect()\n    \ntrain=train.loc[~train.investment_id.isin(outlier_investments)].reset_index(drop=True)\ntrain.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Strange behaviour of some features according to time id\n\n### f_1","metadata":{}},{"cell_type":"code","source":"def get_unique_total_values_ratio(df, feature, plot=False, return_result=False):\n    df_date_group = df.groupby('time_id').agg({feature: [lambda x: len(x.unique()), 'count']})\n    df_date_group.columns.set_levels(['len_unique','count'], level=1,inplace=True)\n    df_date_group['unique_count_ratio'] = df_date_group[feature]['len_unique']/df_date_group[feature]['count']\n    df_date_group['num_or_cat'] = df_date_group['unique_count_ratio'].apply(lambda x: 1 if x > 0.5 else 0)\n    if plot:\n        fig = plt.figure(figsize=(14, 6))\n        ax = fig.add_subplot(111)\n        plt.plot(df_date_group.index, df_date_group['unique_count_ratio'] , label=\"Number of unique investments\")\n    if return_result:\n        return df_date_group['num_or_cat']\n\nget_unique_total_values_ratio(train, 'f_1', plot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### f_6","metadata":{}},{"cell_type":"code","source":"get_unique_total_values_ratio(train, 'f_6', plot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### f_7","metadata":{}},{"cell_type":"code","source":"num_or_cat = get_unique_total_values_ratio(train, 'f_7', plot=True, return_result=True)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add time_id features","metadata":{}},{"cell_type":"code","source":"time_id_df = (\n    train[[\"investment_id\", \"time_id\"]]\n    .groupby(\"investment_id\")\n    .agg([\"min\", \"max\", \"count\", np.ptp])\n    .assign(\n        time_span=lambda x: x.time_id.ptp,\n        time_count=lambda x: x.time_id[\"count\"]\n    )\n    .drop(columns=\"ptp\", level=1)\n    .reset_index()\n)\ntime_id_df.head(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Merge time features with train dataframe","metadata":{}},{"cell_type":"code","source":"train = train.merge(time_id_df.drop(columns=\"time_id\", level=0).droplevel(level=1, axis=1), on=\"investment_id\", how='left')\ntrain[[\"time_span\", \"time_count\"]].hist(bins=args.num_bins, figsize=(16,12), sharex=True, layout=(2,1))\nmax_time_span=time_id_df.time_id[\"max\"].max()\noutlier_investments = time_id_df.loc[time_id_df.time_id[\"count\"]<32, \"investment_id\"].to_list()\ndel time_id_df\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add cluster feature","metadata":{}},{"cell_type":"code","source":"inv_id_to_cluster = pd.read_pickle('../input/ubiquant-lgbm-models/clustered_inv_index.pkl')\n\ntrain = train.merge(inv_id_to_cluster, how='left', on='investment_id')\ntrain.cluster.fillna(0, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add time group feature","metadata":{}},{"cell_type":"code","source":"train.loc[(train['time_id'] < 910), 'group'] = 0\ntrain.loc[(train['time_id'] >= 910) & (train['time_id'] < 970), 'group'] = 1\ntrain.loc[(train['time_id'] >= 970) & (train['time_id'] < 1030), 'group'] = 2\ntrain.loc[(train['time_id'] >= 1030) & (train['time_id'] < 1090), 'group'] = 3\ntrain.loc[(train['time_id'] >= 1090) & (train['time_id'] < 1150), 'group'] = 4\ntrain.loc[(train['time_id'] >= 1150), 'group'] = 5\ntrain['group'] = train['group'].astype(np.int16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add feature that shows if some features are categorical or numerical for the current time_id","metadata":{}},{"cell_type":"code","source":"# add numerical/categorical flag\ntrain = train.merge(num_or_cat, how='left', on='time_id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Some features are not stationary. Let's make a rank transformation by time_id to make them stationary","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train['f_74']  = train[['time_id', 'f_74']].groupby('time_id').rank(pct=True)\n# train['f_142']  = train[['time_id', 'f_142']].groupby('time_id').rank(pct=True)\n# train['f_63']  = train[['time_id', 'f_63']].groupby('time_id').rank(pct=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make combinations for some features and drop not necessary features.","metadata":{}},{"cell_type":"code","source":"cat_features = ['num_or_cat', 'cluster']\nnum_features = list(train.filter(like=\"f_\").columns)\nfeatures = num_features + cat_features\n\n# make feature combinations\ncombination_features = [\"f_231-f_250\", \"f_118-f_280\", \"f_155-f_297\", \"f_25-f_237\", \"f_179-f_265\", \"f_119-f_270\", \"f_71-f_197\", \"f_21-f_65\"]\nfor f in combination_features:\n    f1, f2 = f.split(\"-\")\n    train[f] = train[f1] + train[f2]\nfeatures += combination_features\n\n# drop unnecessary features\n# to_drop = [\"f_148\", \"f_72\", \"f_49\", \"f_205\", \"f_228\", \"f_97\", \"f_262\", \"f_258\"]\n# features = list(sorted(set(features).difference(set(to_drop))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reduce memory usage.","metadata":{}},{"cell_type":"code","source":"%%time\n\ntrain = reduce_mem_usage(train)\ntrain[[\"investment_id\", \"time_id\"]] = train[[\"investment_id\", \"time_id\"]].astype(np.uint16)\n# train=train.drop(columns=[\"row_id\"]+to_drop)\n\nif args.cv_method==\"stratified\":\n    train[\"fold\"] = train[\"fold\"].astype(np.uint8)\ngc.collect()\n#features += [\"time_id\"] # https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302429\nfeatures_backup = features.copy()\nlen(features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train\n\n### Set scoring and training functions","metadata":{}},{"cell_type":"code","source":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\n# TODO: replace with feval_pearsonr\ndef feval_rmse(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'rmse', rmse(y_true, y_pred), False\n\n# https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302480\ndef feval_pearsonr(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'pearsonr', pearsonr(y_true, y_pred)[0], True\n\n# https://www.kaggle.com/gogo827jz/jane-street-supervised-autoencoder-mlp?scriptVersionId=73762661\n# weighted average as per Donate et al.'s formula\n# https://doi.org/10.1016/j.neucom.2012.02.053\n# [0.0625, 0.0625, 0.125, 0.25, 0.5] for 5 fold\ndef weighted_average(a):\n    w = []\n    n = len(a)\n    for j in range(1, n + 1):\n        j = 2 if j == 1 else j\n        w.append(1 / (2**(n + 1 - j)))\n    return np.average(a, weights = w)\n\ndef run(info):    \n    # hyperparams from: https://www.kaggle.com/valleyzw/ubiquant-lgbm-optimization\n    params = {\n        'learning_rate':0.05,\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        'boosting_type': \"gbdt\",\n        'verbosity': -1,\n        'n_jobs': -1, \n        'seed': args.seed,\n#         'lambda_l1': 0.03627602394442367, \n#         'lambda_l2': 0.43523855951142926, \n#         'num_leaves': 114, \n#         'feature_fraction': 0.9505625064462319, \n#         'bagging_fraction': 0.9785558707339647, \n#         'bagging_freq': 7, \n#         'max_depth': -1, \n#         'max_bin': 501, \n#         'min_data_in_leaf': 374,\n        'n_estimators': 1000, \n    }\n    \n    y = train['target']\n    train['preds'] = -1000\n    scores = defaultdict(list)\n    features_importance= []\n    \n    def run_single_fold(fold, trn_ind, val_ind):\n        train_dataset = lgb.Dataset(train.loc[trn_ind, features], y.loc[trn_ind], categorical_feature=cat_features)\n        valid_dataset = lgb.Dataset(train.loc[val_ind, features], y.loc[val_ind], categorical_feature=cat_features)\n        model = lgb.train(\n            params,\n            train_set = train_dataset, \n            valid_sets = [train_dataset, valid_dataset], \n            verbose_eval=50,\n            early_stopping_rounds=50,\n            feval = feval_pearsonr\n        )\n        joblib.dump(model, f'lgbm_seed{args.seed}_{fold}_{info}.pkl')\n        preds = model.predict(train.loc[val_ind, features])\n        train.loc[val_ind, \"preds\"] = preds\n        scores[\"rmse\"].append(rmse(y.loc[val_ind], preds))\n        scores[\"pearsonr\"].append(pearsonr(y.loc[val_ind], preds)[0])\n        fold_importance_df= pd.DataFrame({'feature': features, 'importance': model.feature_importance(), 'fold': fold})\n        features_importance.append(fold_importance_df)\n        del train_dataset, valid_dataset, model\n        gc.collect()\n        \n    time_thresh = round(train.time_id.max() * 0.9)\n    trn_ind, val_ind = train.time_id < time_thresh, train.time_id >= time_thresh\n    print(f\"train length: {trn_ind.sum()}, valid length: {val_ind.sum()}\")\n    run_single_fold(\"single\", trn_ind, val_ind)\n        \n    print(f\"lgbm {info} mean rmse: {np.mean(scores['rmse'])}, mean pearsonr: {np.mean(scores['pearsonr'])}\")\n    if \"time\" in args.cv_method:\n        print(f\"lgbm {info} {args.folds} folds weighted mean rmse: {weighted_average(scores['rmse'])}, weighted mean pearsonr: {weighted_average(scores['pearsonr'])}\")\n    train.filter(regex=r\"^(?!f_).*\").to_csv(f\"preds_{info}.csv\", index=False)\n    return pd.concat(features_importance, axis=0), scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train with deafault params","metadata":{}},{"cell_type":"code","source":"%%time\n\nif args.train:\n    info = \"\"\n    features_importance, scores = run(info=info)\n    df = train[[\"target\", \"preds\", \"time_id\"]].query(\"preds!=-1000\")\n    score = df.groupby(\"time_id\").apply(lambda x: pearsonr(x.target, x.preds)[0]).mean()\n    print(f\"lgbm {info} {args.cv_method} {args.folds} folds mean rmse: {rmse(df.target, df.preds):.4f}, mean pearsonr: {pearsonr(df.target, df.preds)[0]:.4f}, mean pearsonr by time_id: {score:.4f}\")\n\n    features_importance.to_csv(f\"features_importance_{info}.csv\", index=False)\n\n    plt.figure(figsize=(16, 10))\n    plt.subplot(1,2,1)\n    sns.barplot(x=\"importance\", y=\"feature\", data=features_importance.sort_values('importance', ascending=False).head(50))\n    plt.title(f'Head LightGBM Features {info} (avg over {args.folds} folds)')\n    plt.subplot(1,2,2)\n    sns.barplot(x=\"importance\", y=\"feature\", data=features_importance.sort_values('importance', ascending=False).tail(50))\n    plt.title(f'Tail LightGBM Features {info} (avg over {args.folds} folds)')\n    plt.tight_layout()\n    plt.show()\n    del df\n\ndel train\ngc.collect()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-13T12:53:32.008338Z","iopub.execute_input":"2022-03-13T12:53:32.008565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_importance.sort_values('importance', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Default train result (TR)","metadata":{}},{"cell_type":"code","source":"# Training until validation scores don't improve for 50 rounds\n# [50]\ttraining's rmse: 0.913482\ttraining's pearsonr: 0.14288\tvalid_1's rmse: 0.893281\tvalid_1's pearsonr: 0.120359\n# [100]\ttraining's rmse: 0.910073\ttraining's pearsonr: 0.169385\tvalid_1's rmse: 0.892575\tvalid_1's pearsonr: 0.123489\n# [150]\ttraining's rmse: 0.907409\ttraining's pearsonr: 0.188283\tvalid_1's rmse: 0.892045\tvalid_1's pearsonr: 0.127115\n# [200]\ttraining's rmse: 0.905124\ttraining's pearsonr: 0.203294\tvalid_1's rmse: 0.8918\tvalid_1's pearsonr: 0.128508\n# [250]\ttraining's rmse: 0.903073\ttraining's pearsonr: 0.215995\tvalid_1's rmse: 0.891592\tvalid_1's pearsonr: 0.129926\n# [300]\ttraining's rmse: 0.90115\ttraining's pearsonr: 0.227362\tvalid_1's rmse: 0.89147\tvalid_1's pearsonr: 0.130731\n# [350]\ttraining's rmse: 0.899383\ttraining's pearsonr: 0.237686\tvalid_1's rmse: 0.891368\tvalid_1's pearsonr: 0.131481\n# [400]\ttraining's rmse: 0.897813\ttraining's pearsonr: 0.246406\tvalid_1's rmse: 0.891264\tvalid_1's pearsonr: 0.132314\n# [450]\ttraining's rmse: 0.896316\ttraining's pearsonr: 0.254619\tvalid_1's rmse: 0.891225\tvalid_1's pearsonr: 0.132649\n# [500]\ttraining's rmse: 0.894949\ttraining's pearsonr: 0.262003\tvalid_1's rmse: 0.891213\tvalid_1's pearsonr: 0.132787\n# [550]\ttraining's rmse: 0.893638\ttraining's pearsonr: 0.269038\tvalid_1's rmse: 0.891208\tvalid_1's pearsonr: 0.132892\n# Early stopping, best iteration is:\n# [521]\ttraining's rmse: 0.894411\ttraining's pearsonr: 0.264808\tvalid_1's rmse: 0.891164\tvalid_1's pearsonr: 0.133218\n# lgbm  mean rmse: 0.8911636259685726, mean pearsonr: 0.13321790059518845\n# lgbm  single 5 folds mean rmse: 0.8912, mean pearsonr: 0.1332, mean pearsonr by time_id: 0.1323","metadata":{"execution":{"iopub.status.busy":"2022-03-12T18:12:09.854578Z","iopub.execute_input":"2022-03-12T18:12:09.855221Z","iopub.status.idle":"2022-03-12T18:12:09.859577Z","shell.execute_reply.started":"2022-03-12T18:12:09.855187Z","shell.execute_reply":"2022-03-12T18:12:09.858989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TR With cluster (lower than default but cluster feature is very important for LGBM)","metadata":{}},{"cell_type":"code","source":"# Training until validation scores don't improve for 50 rounds\n# [50]\ttraining's rmse: 0.913453\ttraining's pearsonr: 0.143255\tvalid_1's rmse: 0.893224\tvalid_1's pearsonr: 0.121068\n# [100]\ttraining's rmse: 0.910018\ttraining's pearsonr: 0.169897\tvalid_1's rmse: 0.892493\tvalid_1's pearsonr: 0.124274\n# [150]\ttraining's rmse: 0.907276\ttraining's pearsonr: 0.189527\tvalid_1's rmse: 0.892174\tvalid_1's pearsonr: 0.125918\n# [200]\ttraining's rmse: 0.905002\ttraining's pearsonr: 0.204151\tvalid_1's rmse: 0.89183\tvalid_1's pearsonr: 0.128179\n# [250]\ttraining's rmse: 0.902822\ttraining's pearsonr: 0.217748\tvalid_1's rmse: 0.891734\tvalid_1's pearsonr: 0.128604\n# [300]\ttraining's rmse: 0.900959\ttraining's pearsonr: 0.228655\tvalid_1's rmse: 0.891533\tvalid_1's pearsonr: 0.130155\n# [350]\ttraining's rmse: 0.899196\ttraining's pearsonr: 0.23879\tvalid_1's rmse: 0.891401\tvalid_1's pearsonr: 0.131188\n# [400]\ttraining's rmse: 0.897567\ttraining's pearsonr: 0.247953\tvalid_1's rmse: 0.891278\tvalid_1's pearsonr: 0.132196\n# [450]\ttraining's rmse: 0.896013\ttraining's pearsonr: 0.256605\tvalid_1's rmse: 0.891292\tvalid_1's pearsonr: 0.132096\n# Early stopping, best iteration is:\n# [410]\ttraining's rmse: 0.897249\ttraining's pearsonr: 0.249719\tvalid_1's rmse: 0.891244\tvalid_1's pearsonr: 0.132481\n# lgbm  mean rmse: 0.8912439433647903, mean pearsonr: 0.13248139140698012\n# lgbm  single 5 folds mean rmse: 0.8912, mean pearsonr: 0.1325, mean pearsonr by time_id: 0.1313","metadata":{"execution":{"iopub.status.busy":"2022-03-12T18:12:09.860791Z","iopub.execute_input":"2022-03-12T18:12:09.861314Z","iopub.status.idle":"2022-03-12T18:12:09.876098Z","shell.execute_reply.started":"2022-03-12T18:12:09.861276Z","shell.execute_reply":"2022-03-12T18:12:09.875308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TR without feature drop ","metadata":{}},{"cell_type":"code","source":"# train length: 2730227, valid length: 404480\n# Training until validation scores don't improve for 50 rounds\n# [50]\ttraining's rmse: 0.913452\ttraining's pearsonr: 0.143264\tvalid_1's rmse: 0.893225\tvalid_1's pearsonr: 0.121058\n# [100]\ttraining's rmse: 0.91007\ttraining's pearsonr: 0.169496\tvalid_1's rmse: 0.892498\tvalid_1's pearsonr: 0.124362\n# [150]\ttraining's rmse: 0.907347\ttraining's pearsonr: 0.188877\tvalid_1's rmse: 0.892111\tvalid_1's pearsonr: 0.126463\n# [200]\ttraining's rmse: 0.905042\ttraining's pearsonr: 0.204022\tvalid_1's rmse: 0.891882\tvalid_1's pearsonr: 0.12772\n# [250]\ttraining's rmse: 0.903009\ttraining's pearsonr: 0.2164\tvalid_1's rmse: 0.891692\tvalid_1's pearsonr: 0.129003\n# [300]\ttraining's rmse: 0.901035\ttraining's pearsonr: 0.228094\tvalid_1's rmse: 0.891449\tvalid_1's pearsonr: 0.130895\n# [350]\ttraining's rmse: 0.899262\ttraining's pearsonr: 0.238252\tvalid_1's rmse: 0.891331\tvalid_1's pearsonr: 0.131798\n# [400]\ttraining's rmse: 0.897601\ttraining's pearsonr: 0.247844\tvalid_1's rmse: 0.891203\tvalid_1's pearsonr: 0.13283\n# [450]\ttraining's rmse: 0.896149\ttraining's pearsonr: 0.255823\tvalid_1's rmse: 0.891127\tvalid_1's pearsonr: 0.133456\n# [500]\ttraining's rmse: 0.894761\ttraining's pearsonr: 0.263436\tvalid_1's rmse: 0.891047\tvalid_1's pearsonr: 0.134127\n# [550]\ttraining's rmse: 0.893423\ttraining's pearsonr: 0.270585\tvalid_1's rmse: 0.891053\tvalid_1's pearsonr: 0.134128\n# Early stopping, best iteration is:\n# [503]\ttraining's rmse: 0.894681\ttraining's pearsonr: 0.263853\tvalid_1's rmse: 0.891033\tvalid_1's pearsonr: 0.134244\n# lgbm  mean rmse: 0.8910329881660993, mean pearsonr: 0.13424414982435046\n# lgbm  single 5 folds mean rmse: 0.8910, mean pearsonr: 0.1342, mean pearsonr by time_id: 0.1331","metadata":{"execution":{"iopub.status.busy":"2022-03-12T18:12:09.877102Z","iopub.execute_input":"2022-03-12T18:12:09.877713Z","iopub.status.idle":"2022-03-12T18:12:09.891302Z","shell.execute_reply.started":"2022-03-12T18:12:09.877676Z","shell.execute_reply":"2022-03-12T18:12:09.890489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TR without feature combination","metadata":{}},{"cell_type":"code","source":"# train length: 2730227, valid length: 404480\n# Training until validation scores don't improve for 50 rounds\n# [50]\ttraining's rmse: 0.913441\ttraining's pearsonr: 0.144667\tvalid_1's rmse: 0.893187\tvalid_1's pearsonr: 0.122953\n# [100]\ttraining's rmse: 0.910032\ttraining's pearsonr: 0.170463\tvalid_1's rmse: 0.892406\tvalid_1's pearsonr: 0.125743\n# [150]\ttraining's rmse: 0.907376\ttraining's pearsonr: 0.189019\tvalid_1's rmse: 0.892074\tvalid_1's pearsonr: 0.127048\n# [200]\ttraining's rmse: 0.905052\ttraining's pearsonr: 0.203975\tvalid_1's rmse: 0.89172\tvalid_1's pearsonr: 0.129288\n# [250]\ttraining's rmse: 0.902969\ttraining's pearsonr: 0.216697\tvalid_1's rmse: 0.891548\tvalid_1's pearsonr: 0.130297\n# [300]\ttraining's rmse: 0.900992\ttraining's pearsonr: 0.228486\tvalid_1's rmse: 0.891473\tvalid_1's pearsonr: 0.130701\n# [350]\ttraining's rmse: 0.89924\ttraining's pearsonr: 0.238513\tvalid_1's rmse: 0.891393\tvalid_1's pearsonr: 0.131277\n# [400]\ttraining's rmse: 0.897604\ttraining's pearsonr: 0.247669\tvalid_1's rmse: 0.891348\tvalid_1's pearsonr: 0.131625\n# [450]\ttraining's rmse: 0.896106\ttraining's pearsonr: 0.256053\tvalid_1's rmse: 0.89128\tvalid_1's pearsonr: 0.132194\n# [500]\ttraining's rmse: 0.894739\ttraining's pearsonr: 0.263327\tvalid_1's rmse: 0.891271\tvalid_1's pearsonr: 0.132316\n# [550]\ttraining's rmse: 0.893425\ttraining's pearsonr: 0.270405\tvalid_1's rmse: 0.891243\tvalid_1's pearsonr: 0.132612\n# [600]\ttraining's rmse: 0.892154\ttraining's pearsonr: 0.276971\tvalid_1's rmse: 0.891234\tvalid_1's pearsonr: 0.132757\n# Early stopping, best iteration is:\n# [563]\ttraining's rmse: 0.893105\ttraining's pearsonr: 0.272048\tvalid_1's rmse: 0.891207\tvalid_1's pearsonr: 0.132923\n# lgbm  mean rmse: 0.8912068731130676, mean pearsonr: 0.13292278780177647\n# lgbm  single 5 folds mean rmse: 0.8912, mean pearsonr: 0.1329, mean pearsonr by time_id: 0.1322","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TR with time count","metadata":{}},{"cell_type":"code","source":"# Training until validation scores don't improve for 50 rounds\n# [50]\ttraining's rmse: 0.913449\ttraining's pearsonr: 0.143344\tvalid_1's rmse: 0.893183\tvalid_1's pearsonr: 0.121118\n# [100]\ttraining's rmse: 0.910067\ttraining's pearsonr: 0.169453\tvalid_1's rmse: 0.89261\tvalid_1's pearsonr: 0.122867\n# [150]\ttraining's rmse: 0.90745\ttraining's pearsonr: 0.187861\tvalid_1's rmse: 0.89232\tvalid_1's pearsonr: 0.124176\n# [200]\ttraining's rmse: 0.905024\ttraining's pearsonr: 0.20412\tvalid_1's rmse: 0.892068\tvalid_1's pearsonr: 0.125852\n# [250]\ttraining's rmse: 0.90295\ttraining's pearsonr: 0.216915\tvalid_1's rmse: 0.891933\tvalid_1's pearsonr: 0.126738\n# [300]\ttraining's rmse: 0.900969\ttraining's pearsonr: 0.228888\tvalid_1's rmse: 0.891775\tvalid_1's pearsonr: 0.128043\n# [350]\ttraining's rmse: 0.899165\ttraining's pearsonr: 0.239334\tvalid_1's rmse: 0.891638\tvalid_1's pearsonr: 0.129201\n# [400]\ttraining's rmse: 0.897572\ttraining's pearsonr: 0.248159\tvalid_1's rmse: 0.891583\tvalid_1's pearsonr: 0.129724\n# [450]\ttraining's rmse: 0.895988\ttraining's pearsonr: 0.256667\tvalid_1's rmse: 0.891514\tvalid_1's pearsonr: 0.130402\n# [500]\ttraining's rmse: 0.894583\ttraining's pearsonr: 0.264228\tvalid_1's rmse: 0.891546\tvalid_1's pearsonr: 0.130238\n# Early stopping, best iteration is:\n# [460]\ttraining's rmse: 0.895723\ttraining's pearsonr: 0.258197\tvalid_1's rmse: 0.891487\tvalid_1's pearsonr: 0.130623\n# lgbm  mean rmse: 0.8914867840613973, mean pearsonr: 0.13062282948405346\n# lgbm  single 5 folds mean rmse: 0.8915, mean pearsonr: 0.1306, mean pearsonr by time_id: 0.1296","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimize LGBM with Optuna","metadata":{}},{"cell_type":"code","source":"import optuna\nimport warnings\nwarnings.filterwarnings(\"ignore\", module=\"lightgbm\")\nfrom sklearn.metrics import mean_squared_error\n\n# FYI: Objective functions can take additional arguments\n# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\ndef objective(trial):\n    # https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302480\n    def feval_pearsonr(y_pred, lgb_train):\n        y_true = lgb_train.get_label()\n        return 'pearsonr', pearsonr(y_true, y_pred)[0], True\n    \n    time_thresh = round(train.time_id.max() * 0.9)\n    trn_ind, val_ind = train.time_id < time_thresh, train.time_id >= time_thresh\n    \n    X_train, y_train = train.loc[trn_ind, features], train.loc[trn_ind, 'target']\n    X_val, y_val = train.loc[val_ind, features], train.loc[val_ind, 'target']\n    \n    dtrain = lgb.Dataset(X_train, y_train, categorical_feature=cat_features)\n    dvalid = lgb.Dataset(X_val, y_val, categorical_feature=cat_features)\n\n    param = {\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0,\n        'objective': 'regression',\n        'metric': 'rmse',\n        'seed': args.seed,\n        'verbosity': 0,\n        'boosting_type': 'gbdt', # other options: rf, dart, goss\n        'force_col_wise': False, # Use only with CPU devices\n       \n        'subsample_for_bin': 300000, # Number of data that sampled to construct feature discrete bins; setting this \n                                     # to larger value will give better training result but may increase train time\n        'n_estimators': trial.suggest_int('n_estimators', 500, 1200),      \n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-2, 1e-1),\n        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256), # Max number of leaves in one tree\n        'max_bin': trial.suggest_int('max_bin', 32, 255), # Max number of bins that feature values will be \n                                                           # bucketed in. small number of bins may reduce training \n                                                           # accuracy but may deal with overfitting\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0), # Randomly select a subset of features \n                                                                               # if feature_fraction < 1.0\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0), # Randomly select part of data without \n                                                                               # resampling if bagging_fraction < 1.0\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7), # Perform bagging at every k iteration\n        'min_data_in_leaf': trial.suggest_int('min_child_samples', 5, 100), # Minimal number of data in one leaf\n                                                                            # aliases: min_child_samples, \n        'min_sum_hessian_in_leaf': trial.suggest_float('min_sum_hessian_in_leaf', 1e-4, 1e-1), # Stop trying to split \n                                                                                               # leave if sum of it's\n                                                                                               # hessian less than k\n        'cat_smooth': trial.suggest_float('cat_smooth', 10.0, 100.0), # this can reduce the effect of noises in \n                                                                      # categorical features, especially for \n                                                                      # categories with few data\n    }\n\n    # Add a callback for pruning.\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, 'pearsonr', valid_name='valid_1')\n    gbm = lgb.train(\n        param, \n        train_set = dtrain, \n        valid_sets = [dtrain, dvalid], \n        verbose_eval=100, \n        feval=feval_pearsonr,\n        callbacks=[pruning_callback]\n    )\n\n    y_pred = gbm.predict(X_val)\n    corr = pearsonr(y_val, y_pred)[0]\n    return corr\n\n\nif args.optimize is True:\n    study = optuna.create_study(\n        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"maximize\"\n    )\n    study.optimize(objective, timeout=7*1800)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n        \n    \n    ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-12T09:48:57.256838Z","iopub.status.idle":"2022-03-12T09:48:57.257322Z","shell.execute_reply.started":"2022-03-12T09:48:57.25707Z","shell.execute_reply":"2022-03-12T09:48:57.257096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stacking best LGBM models\n\n### Get best parameters","metadata":{}},{"cell_type":"code","source":"if args.train_l2 is True:\n    # lgb_params = study.trials_dataframe()\n    # lgb_params.to_pickle('LGBM_Optuna_params.pkl')\n    lgb_params = pd.read_pickle('LGBM_Optuna_params.pkl').sort_values('value', ascending=False).head(11)\n\n    param_cols = [c for c in lgb_params.columns if c.startswith('params_')]\n    lgb_params = lgb_params[param_cols]\n\n    best_params = list()\n\n    for idx, row in lgb_params.iterrows():\n        row_dict = {k[7:]: v for k, v in row.items()}\n        row_dict['device'] = 'gpu'\n        row_dict['gpu_platform_id'] = 0\n        row_dict['gpu_device_id'] = 0\n        row_dict['verbosity'] = 0\n        row_dict['max_bin'] = int(row_dict['max_bin'])\n        row_dict['bagging_freq'] = int(row_dict['bagging_freq'])\n        row_dict['min_child_samples'] = int(row_dict['min_child_samples'])\n        row_dict['n_estimators'] = int(row_dict['n_estimators'])\n        row_dict['num_leaves'] = int(row_dict['num_leaves'])\n        best_params.append(row_dict)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-12T09:48:57.258505Z","iopub.status.idle":"2022-03-12T09:48:57.259017Z","shell.execute_reply.started":"2022-03-12T09:48:57.258752Z","shell.execute_reply":"2022-03-12T09:48:57.258778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create metafeatures for the train set","metadata":{}},{"cell_type":"code","source":"if args.train_l2 is True:\n    groups_level2 = [1, 2, 3, 4, 5]\n\n    # That is how we get target for the 2nd level dataset\n    y_train_level2 = train.loc[train.group.isin(groups_level2), 'target']\n\n    # Create 1st level feature matrix\n    X_train, y_train = train[features], train['target']\n    \n    # And here we create 2nd level feature matrix, init it with zeros first\n    X_train_level2 = np.zeros([y_train_level2.shape[0], len(best_params)+1])\n    X_train_level2[:, len(best_params)] = y_train_level2\n\n    meta_index_begin = 0\n    meta_index_end = 0\n\n    # Now fill `X_train_level2` with metafeatures\n    for current_group in tqdm(groups_level2):\n        # split data\n        train_index = X_train.loc[train.group <  current_group].index\n        test_index  = X_train.loc[train.group == current_group].index\n\n        X_train_l2 = X_train.loc[train_index, :]\n        X_test_l2 =  X_train.loc[test_index, :]\n\n        y_train_l2 = y_train[train_index]\n        y_test_l2 =  y_train[test_index]\n\n        meta_index_end += y_test_l2.shape[0]\n\n        print(f\"===================== time group: {current_group} =====================\")\n        \n        # predict metafeatures for each of LGBM regressors\n        for i, params in enumerate(tqdm(best_params)):\n            print(f\"===================== model: {i} =====================\")\n            train_dataset = lgb.Dataset(X_train_l2, y_train_l2, categorical_feature=cat_features)\n            model = lgb.train(\n                params,\n                train_set = train_dataset, \n                valid_sets = [train_dataset], \n                verbose_eval=100,\n                feval = feval_pearsonr\n            )\n#             joblib.dump(model, f'lgbr_{i+1}.pkl')\n            \n            pred = model.predict(X_test_l2)\n            X_train_level2[meta_index_begin:meta_index_end, i] = pred\n\n            del train_dataset, model, pred\n            gc.collect()\n\n        meta_index_begin = meta_index_end\n\n    X_train_level2 = pd.DataFrame(X_train_level2, columns=[f'lgbr_{i+1}' for i in range(len(best_params))]+['target'])\n    X_train_level2.to_pickle('LGBM_X_train_level2.pkl')\n    \n    # train every LGB model on the full dataset and save it\n    for i, params in enumerate(tqdm(best_params)):\n        print(f\"===================== model: {i} =====================\")\n        train_dataset = lgb.Dataset(X_train, y_train, categorical_feature=cat_features)\n        model = lgb.train(\n            params,\n            train_set = train_dataset, \n            valid_sets = [train_dataset], \n            verbose_eval=100,\n            feval = feval_pearsonr\n        )\n        joblib.dump(model, f'lgbr_{i+1}.pkl')\n\n    del X_train, y_train\n    gc.collect()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-12T09:48:57.261218Z","iopub.status.idle":"2022-03-12T09:48:57.261743Z","shell.execute_reply.started":"2022-03-12T09:48:57.261454Z","shell.execute_reply":"2022-03-12T09:48:57.261479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fit LinearRegression for the second layer","metadata":{}},{"cell_type":"code","source":"if args.train_l2 is True:\n    X_train_level2 = joblib.load('../input/ubiquant-lgbm-models/LGBM_X_train_level2.pkl')\n\n    X = X_train_level2.iloc[:, [i for i in range(5)]]\n    y = X_train_level2['target']\n\n    lr = LinearRegression()\n    lr.fit(X, y)\n\n    joblib.dump(lr, 'lr.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T09:48:57.262749Z","iopub.status.idle":"2022-03-12T09:48:57.26309Z","shell.execute_reply.started":"2022-03-12T09:48:57.2629Z","shell.execute_reply":"2022-03-12T09:48:57.262916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"import ubiquant\n\nif.args.inference:\n    env = ubiquant.make_env()  \n    iter_test = env.iter_test()\n\n    lr_model = joblib.load('./lr.pkl')\n    lgbr_models = list(map(joblib.load, sorted(Path(\"../input/ubiquant-lgbm-models\").glob(\"lgbr_*.pkl\"))))\n\n    # https://www.kaggle.com/c/ubiquant-market-prediction/discussion/305353 \n    # When making predictions check if the investment_id was in the train set\n    for (test_df, sample_prediction_df) in iter_test:\n        # get feature combinations\n        for f in combination_features:\n            f1, f2 = f.split(\"-\")\n            test_df[f] = test_df[f1] + test_df[f2]\n\n    #     test_df[\"time_id\"] = test_df.row_id.str.split(\"_\", expand=True)[1].astype(np.uint16) # extract time_id from row_id\n        unqiues = len(test_df['f_7'].unique())\n        if unqiues/len(test_df) > 0.5:\n            test_df['num_or_cat'] = 0\n        else:\n            test_df['num_or_cat'] = 1\n\n        test_df[\"preds\"] = lr_model.predict(np.stack([model.predict(test_df[features_backup]) for model in lgbr_models[:5]], axis=1))\n        sample_prediction_df['target'] = test_df[\"preds\"]\n        env.predict(sample_prediction_df) \n        display(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T09:48:57.264595Z","iopub.status.idle":"2022-03-12T09:48:57.264942Z","shell.execute_reply.started":"2022-03-12T09:48:57.264758Z","shell.execute_reply":"2022-03-12T09:48:57.264775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Further ideas\n\n- drop first time ids from dataset \n- drop turbulence time from dataset\n- standard scale?\n- add total number of investment ids in previous time period\n- add total number of invesment ids in current time period\n- use target lags (1, 5, 7, etc)\n- target EMA\n- try LAMA\n\n- try counting time slots for each investment_id\n- LGBM Imputation\n\n- try custom loss function (for DNN)","metadata":{}},{"cell_type":"markdown","source":"# What works for LGBM CV\n\n- define if feature is numerical of categorical at this time period (num_or_cat)\n- clipping of target (at 0.5% min/max)","metadata":{}}]}